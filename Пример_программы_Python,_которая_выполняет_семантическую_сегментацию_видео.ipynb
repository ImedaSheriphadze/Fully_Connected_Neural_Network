{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNozzugjb/6U/sASPDlefcn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Пример программы **Python**, которая выполняет семантическую сегментацию видео с использованием предварительно обученной модели и предоставляет текстовое описание каждого сегментированного объекта. В этом примере используется архитектура **DeepLabv3+** для семантической сегментации и загружается предварительно обученная модель из зоопарка моделей **TensorFlow**.\n",
        "\n",
        "Для начала установим необходимые библиотеки:"
      ],
      "metadata": {
        "id": "jzZiDY0AirK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "JN-s6T5WIodq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "id": "Zzwb-9nCI7qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "\n",
        "# Загрузите предварительно обученную модель DeepLabv3+ из зоопарка моделей TensorFlow\n",
        "model_url = 'http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz'\n",
        "model_path = '/content/deeplabv3_model.tar.gz'\n",
        "urllib.request.urlretrieve(model_url, model_path)\n",
        "\n",
        "# Извлеките файлы модели\n",
        "import tarfile\n",
        "with tarfile.open(model_path, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "model_dir = './deeplabv3_mnv2_pascal_trainval/'\n",
        "\n",
        "# Загрузите предварительно обученную модель семантической сегментации\n",
        "graph = tf.Graph()\n",
        "with tf.compat.v1.gfile.FastGFile(model_dir + 'frozen_inference_graph.pb', 'rb') as f:\n",
        "    graph_def = tf.compat.v1.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "    with graph.as_default():\n",
        "        tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "# Определите метки классов для сегментации\n",
        "class_labels = ['фон', 'самолет', 'велосипед', 'птица', 'лодка', ...]  # Добавьте больше этикеток в соответствии с вашей моделью\n",
        "\n",
        "# Загрузите видео\n",
        "video_path = '/content/path_to_video.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Получить свойства видео\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Создайте объект VideoWriter для сохранения обработанных кадров\n",
        "output_path = '/content/path_to_video1.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Используйте кодек MP4V, XVID, MJPG, H264. По вашу усмотрению\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Создайте сеанс TensorFlow\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "with tf.compat.v1.Session(graph=graph, config=config) as sess:\n",
        "    # Получить входные и выходные тензоры\n",
        "    input_tensor = sess.graph.get_tensor_by_name('ImageTensor:0')\n",
        "    output_tensor = sess.graph.get_tensor_by_name('SemanticPredictions:0')\n",
        "\n",
        "    # Прокрутите каждый кадр в видео\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Предварительно обработайте кадр\n",
        "        frame = cv2.resize(frame, (513, 513))\n",
        "        frame = frame / 255.0\n",
        "        frame = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Выполнение семантической сегментации\n",
        "        predicted_mask = sess.run(output_tensor, feed_dict={input_tensor: frame})\n",
        "        predicted_mask = np.argmax(predicted_mask, axis=-1)\n",
        "        predicted_mask = predicted_mask[0]\n",
        "\n",
        "        # Создание текстовых описаний для каждого сегментированного объекта\n",
        "        for label in np.unique(predicted_mask):\n",
        "            if label == 0:\n",
        "                continue  # Пропустить фоновую метку\n",
        "\n",
        "            mask = np.uint8(predicted_mask == label)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            object_area = cv2.contourArea(contours[0])\n",
        "\n",
        "            # Отфильтруйте мелкие объекты\n",
        "            if object_area < 500:\n",
        "                continue\n",
        "\n",
        "            # Сгенерировать описание объекта\n",
        "            object_label = class_labels[label]\n",
        "            object_description = f\"В {object_label} присутствует в кадре.\"\n",
        "\n",
        "            # Отображение описания объекта на рамке\n",
        "            cv2.putText(frame, object_description, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        \n",
        "        # Сохраните обработанный кадр в новый видеофайл\n",
        "        out.write(np.uint8(frame))\n",
        "\n",
        "    # Отпустите захват видео и закройте окна\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "q7--R5tMSDum"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот пошаговое описание алгоритма на основе предоставленного кода:\n",
        "\n",
        "1. Установите пакеты **TensorFlow** и **OpenCV**: этот блок кода устанавливает необходимые пакеты с помощью **pip**.\n",
        "\n",
        "2. Загрузите предварительно обученную модель **DeepLabv3+**: код загружает модель **DeepLabv3+** из зоопарка моделей **TensorFlow**, используя указанный URL-адрес, и сохраняет ее по указанному пути к модели.\n",
        "\n",
        "3. Извлеките файлы модели: код извлекает загруженные файлы модели из архива **tar.gz** с помощью модуля **tarfile** и сохраняет их в каталоге **model_dir**.\n",
        "\n",
        "4. Загрузите предварительно обученную модель семантической сегментации: код загружает определение графа предварительно обученной модели из файла **Frozen_inference_graph.pb** и устанавливает его в качестве графа по умолчанию в **TensorFlow**.\n",
        "\n",
        "5. Определите метки классов для сегментации: код определяет список меток классов, используемых для семантической сегментации. Вы можете добавить в список дополнительные метки в зависимости от конкретной модели.\n",
        "\n",
        "6. Загрузите видео: код загружает входное видео из указанного пути к видео с помощью класса **OpenCV VideoCapture**.\n",
        "\n",
        "7. Получить свойства видео: код извлекает ширину, высоту и количество кадров в секунду **(fps)** видео, используя свойства захвата видео **OpenCV**.\n",
        "\n",
        "8. Создать объект **VideoWriter**: код создает объект **VideoWriter** для сохранения обработанных кадров в новый видеофайл. Он указывает выходной путь, кодек **MP4V**, частоту кадров и размер кадра.\n",
        "\n",
        "9. Создайте сеанс **TensorFlow**: код создает сеанс **TensorFlow** с загруженным графом и конфигурацией графического процессора. Это позволяет динамически распределять память графического процессора с помощью **allow_growth**.\n",
        "\n",
        "10. Выполнение семантической сегментации каждого кадра: код считывает каждый кадр из видео с помощью метода **cap.read()** внутри цикла **while**. Если кадр успешно прочитан, он переходит к его обработке.\n",
        "\n",
        "11. Предварительно обработайте кадр: код изменяет размер кадра до требуемого входного размера модели **(513x513)**, нормализует значения пикселей от **0** до **1** и расширяет размеры, чтобы они соответствовали входной форме модели.\n",
        "\n",
        "12. Выполнение семантической сегментации: код пропускает предварительно обработанный кадр через загруженную модель, чтобы получить предсказанную маску сегментации. Он выбирает наиболее вероятную метку для каждого пикселя, находя индекс максимального значения по последней оси.\n",
        "\n",
        "13. Создание текстовых описаний для сегментированных объектов: код перебирает уникальные метки в предсказанной маске и отфильтровывает фоновую метку **(0)**. Для каждой оставшейся метки он создает бинарную маску, находит контуры и вычисляет площадь объекта. Небольшие объекты площадью менее **500** пикселей пропускаются. Он генерирует описание для каждого объекта на основе метки и сохраняет его в **object_description**.\n",
        "\n",
        "14. Отображение описаний объектов на фрейме: код накладывает созданные описания объектов на фрейм с помощью функции **OpenCV putText**, указывая положение, шрифт, цвет и толщину.\n",
        "\n",
        "15. Сохраните обработанный кадр в новый видеофайл: код записывает обработанный кадр в выходной видеофайл с помощью метода **out.write()**.\n",
        "\n",
        "16. Освобождение ресурсов и закрытие окон. После обработки всех кадров код освобождает объекты захвата и записи видео и закрывает все открытые окна.\n",
        "\n",
        "Этот алгоритм выполняет семантическую сегментацию каждого кадра входного видео, генерирует описания объектов и сохраняет обработанные кадры с описаниями в новый видеофайл."
      ],
      "metadata": {
        "id": "fXAuubF1jMPJ"
      }
    }
  ]
}